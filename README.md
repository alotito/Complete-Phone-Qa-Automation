# Automated Call Quality Assurance System

## 1. Overview

This project is a comprehensive, automated system designed to streamline the Call Quality Assurance (QA) process at Global Tech Solutions. It leverages the Google Generative AI (Gemini) model to analyze agent call recordings, store the structured results in a Microsoft SQL Server database, and provide reporting tools for management.

The system is composed of several interconnected Python scripts that handle the entire workflow: from collecting audio files to generating final performance reports.

---

## 2. System Architecture

The workflow is a multi-stage pipeline where each script performs a specific task and prepares data for the next stage.

1.  **File Collection (`PhoneQACopyTool.py`):** The process begins here. The script scans a source directory for agent call recordings (`.wav` files) from the previous calendar week. It identifies the largest files for each agent and copies them into a structured weekly processing directory.

2.  **AI Analysis (`AutoQA.py`):** This is the core engine. It monitors the processing directory for new audio files. Each file is uploaded to the Google AI service for analysis based on a set of defined prompts. It then generates individual and combined summary reports in both `.docx` (for human review) and `.json` (for machine processing) formats. Finally, it emails a summary report to the agent.

3.  **Database Import (`JSON_DB_Importer.py`):** This script scans the processing directory for the `.json` files generated by the AI. It parses the structured data and imports it into the `PhoneQA` MS-SQL database, creating a long-term, queryable record of all QA analyses.

4.  **Management Reporting (`Generate_daily_stats.py`):** This script runs periodically (e.g., daily) to query the database, calculate agent performance scores based on the latest data, and email a summary report in HTML format to a list of managers.

5.  **On-Demand GUI Tool (`report_downloader_app.py`):** A user-friendly desktop application for managers. It connects directly to the database, allowing them to select an agent and a specific report period to download a complete, consolidated QA report in `.docx` format.

![System Flow](https://placehold.co/800x250/f0f0f0/333333?text=File+Collection+->+AI+Analysis+->+DB+Import+->+Reporting)

---

## 3. Core Components (Programs)

### `PhoneQACopyTool.py`
* **Function:** Copies a specified number of the largest audio files for each agent from a source location to the central processing directory for the previous week.
* **Dependencies:** `config.ini`, `ExtList.data`
* **CLI Arguments:**
    * `--date YYYY-MM-DD`: (Optional) Overrides the current date to process a specific week. The date provided should be a Sunday.
    * `--debug`: (Optional) Enables verbose logging to the console.

### `AutoQA.py`
* **Function:** The primary analysis engine. It takes audio files, sends them to the Google AI for evaluation, generates reports, and emails results to agents.
* **Dependencies:** `config.ini`, `ExtList.data`, `prompts/` directory, `google-generativeai`, `python-docx`
* **CLI Arguments:**
    * `--date YYYY-MM-DD`: (Optional) Overrides the current date.
    * `--debug`: (Optional) Enables verbose logging.

### `JSON_DB_Importer.py`
* **Function:** Parses the structured JSON analysis files and imports them into the SQL Server database. It renames processed files to prevent duplicates.
* **Dependencies:** `config.ini`, `ExtList.data`, `pyodbc`
* **CLI Arguments:** None. It automatically finds the latest unprocessed week folder.

### `Generate_daily_stats.py`
* **Function:** A scheduled task that generates and emails a daily performance scorecard to management.
* **Dependencies:** `config.ini`, `pyodbc`
* **CLI Arguments:** None.

### `report_downloader_app.py`
* **Function:** A Tkinter-based GUI application that allows managers to connect to the database and download consolidated reports on demand.
* **Dependencies:** `config.ini`, `pyodbc`, `python-docx`, `tkinter`
* **CLI Arguments:** None. It is a standalone GUI application.

---

## 4. Configuration Files

All scripts are controlled by two central configuration files located in the same directory as the scripts.

### `config.ini`
This is the main configuration file. **It must be named `config.ini` for the scripts to function.**

| Section                    | Key                              | Description                                                                                             |
| -------------------------- | -------------------------------- | ------------------------------------------------------------------------------------------------------- |
| **`[Database]`** | `Server`, `Database`, `User`, `Password` | Credentials for the MS-SQL Server. Used by importer, stats generator, and report downloader.        |
| **`[Prompts]`** | `IndividualPromptFile`           | The default prompt file for individual call analysis. Placed in the `prompts` sub-folder.                 |
|                            | `CombinedPromptFile`             | The prompt file for generating the combined agent summary.                                              |
|                            | `EmailPromptFile`                | The prompt file for generating the body of the email sent to the agent.                                 |
| **`[CopyTool]`** | `FilesToCopyPerExtension`        | The number of the largest `.wav` files to copy for each agent per week.                                 |
|                            | `FilePatterns`                   | Comma-separated list of file types to look for (e.g., `*.wav,*.mp3`).                                   |
| **`[SMTP]`** | `Server`, `Port`, `UID`, etc.    | Settings for the outbound mail server used to send all emails.                                          |
| **`[Paths]`** | `CopySourceRoot`                 | The source UNC path where original call recordings are stored (e.g., `\\server\recordings`).          |
|                            | `ImporterSourceRoot`             | The central processing UNC path where weekly folders are created and reports are stored.                |
|                            | `AutoQALogOutputRoot`            | The base directory where log files for the `AutoQA` script are written.                                 |
| **`[API]`** | `API_Key_B64`                    | The Google Generative AI API key, encoded in Base64.                                                    |
|                            | `ModelName`                      | The specific AI model to use for analysis (e.g., `gemini-1.5-pro-preview-0514`).                          |
| **`[ExtListChecksum]`** | `hash`                           | A SHA256 hash of `ExtList.data`. The copy tool verifies this to ensure the file is not tampered with. |
| **`[AutoQA Emails]`** | `FromAddress`, `CCAddresses`     | Email settings for the reports sent to individual agents.                                               |
| **`[Management Report Emails]`** | `From`, `TO`, `CC`               | Email settings for the summary report sent to management.                                               |

### `ExtList.data`
This file maps agent extensions to their details. **It must be named `ExtList.data`**. It is a tab-separated file.

* **Format:**
    ```
    Extension	FullName	EmailAddress	OptionalCustomPromptFile.txt
    ```
* **Columns:**
    1.  **Extension (Required):** The agent's 4-digit phone extension.
    2.  **FullName (Required):** The agent's full name.
    3.  **EmailAddress (Required):** The agent's email address.
    4.  **Custom Prompt (Optional):** The filename of a custom prompt located in the `prompts` folder. If present, `AutoQA.py` will use this prompt for this agent instead of the default `IndividualPromptFile`. If omitted, the default is used.

---

## 5. Setup and Installation

1.  **Clone Repository:**
    ```bash
    git clone <repository_url>
    cd <repository_name>
    ```

2.  **Prerequisites:**
    * Python 3.10 or higher.
    * Access to a Microsoft SQL Server instance and an ODBC driver.
    * Access to the network paths specified in `config.ini`.

3.  **Install Libraries:**
    Create and activate a virtual environment (recommended), then install the required libraries.
    ```bash
    pip install -r requirements.txt
    ```
    **`requirements.txt` contents:**
    ```
    pyodbc
    google-generativeai
    python-docx
    pyinstaller
    ```

4.  **Database Setup:**
    Ensure the database specified in `config.ini` exists. The necessary tables (`Agents`, `IndividualCallAnalyses`, etc.) will be created by the importer script if they don't exist, but the database itself must be created manually. The user in the config file needs `db_datareader` and `db_datawriter` roles.

5.  **Configuration:**
    * Rename `Config.txt` to `config.ini` and `ExtList.txt` to `ExtList.data`.
    * Update `config.ini` with your specific paths, database credentials, API key, and email settings.
    * Populate `ExtList.data` with your agent information.
    * Create a `prompts` sub-folder and add your prompt text files.

---

## 6. Execution and Scheduling

The scripts should be run in a specific order. This process can be automated using Windows Task Scheduler.

1.  **`PhoneQACopyTool.py`:** Run once a week (e.g., every Monday morning) to gather the previous week's calls.
2.  **`AutoQA.py`:** Run after the copy tool to perform the AI analysis. This may take a significant amount of time depending on the number of calls.
3.  **`JSON_DB_Importer.py`:** Run after the AI analysis is complete to import the results into the database.
4.  **`Generate_daily_stats.py`:** Schedule to run daily to send out management reports.

---

## 7. Building the Report Downloader

To create a single `.exe` file for the `report_downloader_app.py` for easy distribution to managers:

1.  Navigate to the project directory in your terminal.
2.  Run the following PyInstaller command:
    ```bash
    pyinstaller --onefile --windowed --add-data "config.ini;." report_downloader_app.py
    ```
3.  The executable will be located in the `dist/` folder.

---

## 8. Logging

Logs for each script are generated in a `logs/` sub-directory, organized by script name. These are the first place to check for errors or to trace the execution flow. The verbosity of console output can be controlled with the `--debug` flag where available.
